#!/usr/bin/env python3

import argparse
import json
import requests
from bs4 import BeautifulSoup
from datetime import datetime
from cyclonedx.model.impact_analysis import ImpactAnalysisAffectedStatus
from cyclonedx.model.vulnerability import Vulnerability, VulnerabilitySource,VulnerabilityScoreSource, VulnerabilityRating, VulnerabilitySeverity, BomTarget, BomTargetVersionRange
"""
Utilities to fetch data from OJVG and convert it to intermediate representations/CycloneDX structure
"""
def fetch_cves(date: str)->list[Vulnerability]:
    return dict_to_vulns(fetch_dicts(date))

def fetch_dicts(date:str):
    cve_text = retrieve_cves_from_internet(date)
    dicts = parse_to_dict(cve_text, date)
    return dicts

def retrieve_cves_from_internet(date: str) -> str:
    # fetch the CVEs for the given date
    url = 'https://openjdk.org/groups/vulnerability/advisories/' + date
    print(url)
    try:
        r = requests.get(url, timeout=5, headers={"User-Agent":"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:109.0) Gecko/20100101 Firefox/109.0",
                                                  "Accept-Language":"en-US,en;q=0.5",
                                                  "Accept-Encoding":"gzip, deflate, br",
                                                  "Accept":"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8",
                                                  "Referer":"http://www.google.com/"})
        print(r)
    except requests.exceptions.ReadTimeout:
        return None
    if r.status_code == 404:
        return None
    resp_text = r.text
    # todo: make this configurable
    with open("data/open_jvg_dump_"+date+".html", "w") as dump:
        dump.write(resp_text)
    return resp_text

def parse_to_cyclone(resp_text: str, date: str) -> list[Vulnerability]:
    dicts = parse_to_dict(resp_text, date)
    return dict_to_vulns(dicts)

def parse_to_dict(resp_text: str, date: str) -> list[dict]:
    if resp_text is None:
        return None
    soup = BeautifulSoup(resp_text, 'html.parser')

    #find the versions affected
    header_string = soup.find(name="p")
    extracted_affected = extract_affected(header_string.text)

    # find the table with the CVEs
    table = soup.find('table', attrs={'class': 'risk-matrix'})

    # find all the rows in the table
    rows = table.find_all('tr')
    dicts = []
    # fetch CVE data from first td in each row
    for row in rows:

        # find the versions in the first row
        header = row.find('th')
        versions = []
        if header is not None:
            component = header.find_next_sibling('th')
            if component.text == 'Component':
                score = component.find_next_sibling('th')
                while (score.find_next_sibling('th') is not None):
                    versions.append(score.find_next_sibling('th').text)
                    score = score.find_next_sibling('th')

        cve = row.find('td')
        if cve is not None:
            id = cve.text
            if(cve.text == "None"):
                continue
            link = cve.find('a')['href']
            componentsTD = cve.find_next_sibling('td')
            component = componentsTD.text.replace('\n', '')
            scoreTD = componentsTD.find_next_sibling('td')
            score = scoreTD.text

            versionCheck = scoreTD
            affected_versions = []
            affected_versions+= extracted_affected #todo - maybe just the extracted ones
            for version in versions:
                versionCheck = versionCheck.find_next_sibling('td')
                if versionCheck.text == 'â€¢':
                    affected_versions.append(int(version))

            
            parsed_data = {}
            parsed_data["id"] = id
            parsed_data["url"] = link
            parsed_data["date"] = date
            parsed_data["component"] = component
            parsed_data["affected"] = affected_versions
            print(json.dumps(parsed_data))
            dicts.append(parsed_data)
            
    return dicts
def dict_to_vulns(dicts: list[dict]) -> list[Vulnerability]:
    vulnerabilities = []
    for parsed_data in dicts:
        affects = BomTarget(
            ref=parsed_data["component"]
        )
        # for v in parsed_data["affected"]:
            #todo: this is not actually true - the affected versions are just for the whole report
            # we need to extract affected versions on a per cve basis, not a per ojvg report basis
            # affects.versions.add(v)
        vuln = Vulnerability(
            id=parsed_data["id"],
            source=VulnerabilitySource(name="National Vulnerability Database", url=parsed_data["url"]),
            #todo: dummy date
            published=datetime.fromisoformat(parsed_data["date"]),
            updated=datetime.fromisoformat(parsed_data["date"]),
            description="",
            recommendation=""
        )
        vuln.affects.add(affects)
        vulnerabilities.append(vuln)
        # print(vuln)
    return vulnerabilities

def extract_affected(header_string: str) -> list[str]:
    header_string = header_string.replace("\r", "").replace("\n"," ")
    # print(header_string)
    affected = []
    start_vulns = "The affected versions are "
    end_vulns = "Please note that defense-in-depth issues"
    if start_vulns not in header_string or end_vulns not in header_string:
        return []
    vulns_sub = header_string[header_string.index(start_vulns)+len(start_vulns):header_string.index(end_vulns)]
    #print(vulns_sub)
    for ver in vulns_sub.split(","):
        ver = ver.strip()
        if "earlier" not in ver:
            affected.append(ver)
    # print(affected)
    return affected

# fetch_cves('2023-01-17')